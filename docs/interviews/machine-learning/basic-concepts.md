# 机器学习基础概念面试题

## 1. 机器学习基础

### Q1: 什么是机器学习？它与传统编程有什么区别？

**答案**:
机器学习是人工智能的一个子领域，它使计算机能够在没有明确编程的情况下学习和改进。

**区别**:
- **传统编程**: 输入数据 + 程序 → 输出结果
- **机器学习**: 输入数据 + 输出结果 → 程序（模型）

**举例**:
- 传统编程：写规则判断邮件是否为垃圾邮件
- 机器学习：用大量邮件样本训练模型自动判断

### Q2: 解释监督学习、无监督学习和强化学习的区别

**答案**:

**监督学习**:
- 使用标记数据训练
- 目标：学习输入到输出的映射
- 例子：分类、回归
- 评估：有明确的正确答案

**无监督学习**:
- 使用无标记数据
- 目标：发现数据中的隐藏模式
- 例子：聚类、降维
- 评估：相对困难

**强化学习**:
- 通过与环境交互学习
- 目标：学习最优策略
- 例子：游戏AI、机器人控制
- 评估：通过累积奖励

### Q3: 什么是过拟合？如何防止过拟合？

**答案**:

**过拟合定义**:
模型在训练数据上表现很好，但在测试数据上表现差，即模型过度适应训练数据。

**防止方法**:
1. **数据层面**:
   - 增加训练数据
   - 数据增强
   - 交叉验证

2. **模型层面**:
   - 降低模型复杂度
   - 早停（Early Stopping）
   - Dropout（神经网络）

3. **正则化**:
   - L1正则化（Lasso）
   - L2正则化（Ridge）
   - 弹性网络（Elastic Net）

4. **集成方法**:
   - Bagging
   - Boosting

### Q4: 解释偏差-方差权衡

**答案**:

**偏差（Bias）**:
- 模型预测值与真实值之间的系统性误差
- 高偏差 → 欠拟合
- 原因：模型过于简单

**方差（Variance）**:
- 模型对训练数据变化的敏感性
- 高方差 → 过拟合
- 原因：模型过于复杂

**权衡关系**:
- 降低偏差会增加方差
- 降低方差会增加偏差
- 目标：找到最优平衡点

**实际应用**:
- 简单模型：高偏差，低方差
- 复杂模型：低偏差，高方差
- 需要根据具体问题选择合适的复杂度

### Q5: 什么是交叉验证？为什么需要交叉验证？

**答案**:

**交叉验证定义**:
将数据集分成多个子集，轮流使用其中一个子集作为验证集，其余作为训练集，多次训练和验证。

**K折交叉验证步骤**:
1. 将数据随机分为K个相等的子集
2. 用K-1个子集训练模型，1个子集验证
3. 重复K次，每次使用不同的验证集
4. 计算K次验证结果的平均值

**为什么需要**:
1. **评估泛化能力**: 了解模型在新数据上的表现
2. **充分利用数据**: 每个样本都参与训练和验证
3. **减少随机性**: 多次验证减少偶然性影响
4. **模型选择**: 比较不同模型的性能
5. **超参数调优**: 选择最优的超参数

### Q6: 解释什么是特征工程？为什么重要？

**答案**:

**特征工程定义**:
从原始数据中提取、构造和选择对机器学习模型有用的特征的过程。

**重要性**:
1. **数据质量决定模型上限**: 好的特征比复杂的算法更重要
2. **提高模型性能**: 合适的特征能显著提升模型效果
3. **降低计算复杂度**: 减少特征维度，提高效率
4. **增强可解释性**: 有意义的特征便于理解模型

**主要内容**:
1. **特征提取**: 从原始数据中提取有用信息
2. **特征构造**: 组合现有特征创建新特征
3. **特征选择**: 选择最重要的特征
4. **特征变换**: 标准化、归一化、编码等

**举例**:
- 文本分类：TF-IDF、词向量
- 图像识别：边缘检测、纹理特征
- 推荐系统：用户行为特征、物品特征

### Q7: 什么是梯度下降？解释其工作原理

**答案**:

**梯度下降定义**:
一种优化算法，通过迭代更新参数来最小化损失函数。

**工作原理**:
1. 初始化参数θ
2. 计算损失函数J(θ)关于θ的梯度∇J(θ)
3. 更新参数：θ = θ - α∇J(θ)
4. 重复步骤2-3直到收敛

**数学公式**:
```
θ_{t+1} = θ_t - α∇J(θ_t)
```

**关键概念**:
- **学习率α**: 控制每次更新的步长
- **梯度**: 损失函数变化最快的方向
- **收敛**: 参数不再显著变化

**变种**:
- **批量梯度下降**: 使用全部数据计算梯度
- **随机梯度下降**: 使用单个样本计算梯度
- **小批量梯度下降**: 使用小批量数据计算梯度

### Q8: 解释什么是正则化？L1和L2正则化的区别

**答案**:

**正则化定义**:
在损失函数中添加惩罚项，防止模型过拟合的技术。

**L1正则化（Lasso）**:
- 惩罚项：λ∑|w_i|
- 特点：产生稀疏解，自动特征选择
- 适用：特征选择、高维数据

**L2正则化（Ridge）**:
- 惩罚项：λ∑w_i²
- 特点：参数收缩，但不为零
- 适用：防止过拟合、数值稳定

**区别对比**:

| 特性 | L1正则化 | L2正则化 |
|------|----------|----------|
| 稀疏性 | 产生稀疏解 | 不产生稀疏解 |
| 特征选择 | 自动选择特征 | 保留所有特征 |
| 计算复杂度 | 较高（不可导） | 较低（可导） |
| 几何解释 | 菱形约束 | 圆形约束 |

**选择建议**:
- 需要特征选择：L1
- 防止过拟合：L2
- 平衡两者：弹性网络

### Q9: 什么是ROC曲线和AUC？如何解释？

**答案**:

**ROC曲线**:
接收者操作特征曲线，以假正率(FPR)为横轴，真正率(TPR)为纵轴。

**关键指标**:
- **TPR (真正率)**: TPR = TP/(TP+FN) = 召回率
- **FPR (假正率)**: FPR = FP/(FP+TN)
- **AUC**: ROC曲线下的面积

**AUC解释**:
- **AUC = 1**: 完美分类器
- **AUC = 0.5**: 随机分类器
- **AUC < 0.5**: 比随机还差
- **AUC = 0.8**: 80%的概率正确区分正负样本

**优势**:
1. **阈值无关**: 不依赖特定分类阈值
2. **类别平衡**: 对类别不平衡不敏感
3. **直观理解**: 面积大小直接反映性能

**使用场景**:
- 二分类问题
- 类别不平衡
- 需要比较不同模型

### Q10: 什么是集成学习？解释Bagging和Boosting的区别

**答案**:

**集成学习定义**:
组合多个弱学习器（基学习器）来构建一个强学习器的技术。

**核心思想**:
- 三个臭皮匠胜过诸葛亮
- 通过组合多个模型提高预测性能
- 减少过拟合风险

**Bagging (Bootstrap Aggregating)**:
- **并行训练**: 同时训练多个基学习器
- **数据采样**: 每个基学习器使用不同的训练子集
- **投票机制**: 最终预测通过投票或平均得出
- **例子**: 随机森林
- **优点**: 减少方差，并行训练

**Boosting**:
- **串行训练**: 依次训练基学习器
- **关注错误**: 后续学习器关注前面学习器的错误
- **加权组合**: 根据性能给不同学习器分配权重
- **例子**: AdaBoost, Gradient Boosting
- **优点**: 减少偏差，通常性能更好

**对比**:

| 特性 | Bagging | Boosting |
|------|---------|----------|
| 训练方式 | 并行 | 串行 |
| 关注点 | 减少方差 | 减少偏差 |
| 数据使用 | 有放回采样 | 全部数据 |
| 过拟合风险 | 较低 | 较高 |
| 训练速度 | 快 | 慢 |

**选择建议**:
- 高方差问题：Bagging
- 高偏差问题：Boosting
- 需要快速训练：Bagging
- 追求最佳性能：Boosting
