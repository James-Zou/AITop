# 大模型架构与算法面试题库

## 基础概念

### 1. 主流开源大模型体系
**问题**: 主流的开源大模型体系有哪些，并简要介绍它们的特点？

**答案要点**:
- **GPT系列**: 基于Transformer的生成式预训练模型，采用decoder-only架构，擅长文本生成
- **BERT**: 双向编码器表示，采用encoder架构，擅长语言理解和表征学习
- **T5**: 文本到文本转换Transformer，统一多任务处理框架
- **RoBERTa**: BERT的改进版本，通过更大规模数据和更长训练时间提升性能
- **XLNet**: 自回归预训练模型，能建模全局依赖关系

### 2. 语言模型架构对比
**问题**: 解释prefix LM和causal LM的区别，并给出实际应用案例。

**答案要点**:
- **Prefix LM**: 双向上下文预测，可用于理解任务
- **Causal LM**: 仅基于历史信息预测，如GPT系列，适用于生成任务
- **应用**: BERT使用prefix LM进行理解，GPT使用causal LM进行生成

### 3. 模型架构差异
**问题**: 简述GPT和BERT模型的主要区别，以及它们各自的优势。

**答案要点**:
- **GPT**: decoder-only，生成式，擅长文本生成和续写
- **BERT**: encoder-only，判别式，擅长语言理解和分类任务
- **训练目标**: GPT使用自回归，BERT使用掩码语言模型

## 高级特性

### 4. 涌现能力
**问题**: 如何定义和理解大模型中的"涌现能力"，并举例说明？

**答案要点**:
- **定义**: 模型在处理复杂任务时展现出的未被直接编程的高级技能
- **原因**: 数据量增加、计算能力提升、模型规模扩大、训练方法改进
- **例子**: 代码生成、数学推理、多语言理解、创意写作

### 5. 生成式vs判别式模型
**问题**: 描述生成式语言模型的工作原理，并解释它如何不同于判别式模型。

**答案要点**:
- **生成式**: 学习数据分布，生成新样本，如GPT、VAE
- **判别式**: 学习决策边界，区分不同类别，如分类器
- **训练目标**: 生成式最大化似然，判别式最小化分类误差

## 训练与优化

### 6. 灾难性遗忘
**问题**: 大模型训练中如何应对"灾难性遗忘"问题？

**答案要点**:
- **经验回放**: 保留旧任务数据，定期重训练
- **正则化方法**: 弹性权重巩固(EWC)、知识蒸馏
- **参数隔离**: 为不同任务分配独立参数
- **持续学习**: 渐进式学习新任务

### 7. 微调策略
**问题**: 在微调大模型时，选择合适的数据集和微调策略至关重要，请阐述其考虑因素。

**答案要点**:
- **数据质量**: 相关性、规模、多样性、标注准确性
- **微调参数**: 学习率、批次大小、迭代次数、早停策略
- **方法选择**: 全量微调、LoRA、QLoRA、Adapter

## 模型评估

### 8. 性能指标
**问题**: 如何评价大模型的性能指标，除了准确率之外，还有哪些关键指标？

**答案要点**:
- **语言建模**: Perplexity、困惑度
- **生成质量**: BLEU、ROUGE、METEOR
- **分类任务**: F1分数、AUC-ROC、精确率、召回率
- **效率指标**: 推理速度、内存占用、模型大小

## 偏见与公平性

### 9. 模型偏见
**问题**: 哪些因素可能导致大模型出现偏见，如何减轻这种偏见？

**答案要点**:
- **偏见来源**: 数据偏差、算法设计、训练过程强化
- **减轻方法**: 数据多样化、去偏算法、公平性评估、对抗训练
- **评估工具**: 偏见检测指标、公平性测试集

## 生成模型架构

### 10. 生成模型对比
**问题**: 解释并比较AE、VAE、GAN的工作机制及其差异。

**答案要点**:
- **AE**: 编码器-解码器结构，重构损失
- **VAE**: 变分推断，最大化似然下界，生成连续数据
- **GAN**: 对抗训练，生成器和判别器博弈，生成高质量样本
- **应用场景**: AE用于降维，VAE用于生成，GAN用于高质量生成

## 技术深度

### 11. 模型压缩
**问题**: 模型压缩技术在实际部署中有何挑战？

**答案要点**:
- **量化**: 降低精度，可能影响性能
- **剪枝**: 移除冗余参数，需要重新训练
- **蒸馏**: 小模型学习大模型知识，效果可能下降
- **挑战**: 精度损失、硬件兼容性、部署复杂度

### 12. 分布式训练
**问题**: 大模型分布式训练的关键技术有哪些？

**答案要点**:
- **数据并行**: 模型复制，数据分片
- **模型并行**: 模型分片，张量并行
- **流水线并行**: 层间并行，减少通信开销
- **混合精度**: FP16训练，减少内存占用

## 实际应用

### 13. 模型选择
**问题**: 如何选择适合的大模型开源框架？

**答案要点**:
- **任务类型**: 生成vs理解，单语言vs多语言
- **资源限制**: 计算资源、内存、推理速度要求
- **部署环境**: 云端vs边缘，实时性要求
- **维护成本**: 社区支持、文档质量、更新频率

### 14. 推理优化
**问题**: 大模型推理优化的主要方法有哪些？

**答案要点**:
- **批处理**: 批量处理多个请求
- **缓存机制**: KV缓存、注意力缓存
- **动态批处理**: 根据请求动态调整批次大小
- **硬件优化**: GPU推理、专用芯片、量化推理