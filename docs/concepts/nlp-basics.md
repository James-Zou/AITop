# 自然语言处理基础概念

## 1. 文本预处理

### 分词
将连续的文本序列切分成有意义的词汇单元。

**中文分词挑战**:
- 无天然分隔符
- 歧义性
- 新词识别
- 未登录词

**分词方法**:
- 基于词典：最大匹配法、双向匹配法
- 基于统计：N-gram模型、HMM模型
- 基于机器学习：CRF、SVM
- 基于深度学习：BiLSTM-CRF、BERT

### 词性标注
为句子中的每个词确定其语法类别。

**常用方法**:
- 基于规则：词典查找、规则匹配
- 基于统计：HMM、最大熵模型
- 基于机器学习：SVM、CRF
- 基于深度学习：BiLSTM-CRF、BERT

### 命名实体识别
从文本中识别和分类命名实体。

**实体类型**:
- 人名(PER)
- 地名(LOC)
- 机构名(ORG)
- 时间(TIME)
- 货币(MONEY)
- 百分比(PERCENT)

## 2. 语言模型

### 定义
语言模型是计算一个句子或文本序列概率的模型。

**数学表示**:
```
P(w1, w2, ..., wn) = P(w1) × P(w2|w1) × ... × P(wn|w1, w2, ..., wn-1)
```

### 模型类型
- N-gram模型：基于统计的语言模型
- 神经网络语言模型：基于神经网络
- 预训练语言模型：BERT、GPT等

## 3. 词向量

### 定义
词向量是将词语映射到高维向量空间的技术。

### 表示方法
- 独热编码：只有一个位置为1
- 分布式表示：稠密向量，每个维度都有意义

### 生成方法
- 基于统计：共现矩阵、SVD分解
- Word2Vec：Skip-gram、CBOW
- GloVe：结合全局和局部信息
- FastText：考虑子词信息
- 上下文词向量：ELMo、BERT

## 4. 注意力机制

### 定义
注意力机制是一种让模型能够关注输入序列中不同位置信息的机制。

### 数学表示
```
Attention(Q, K, V) = softmax(QK^T/√d_k)V
```

### 类型
- 自注意力：查询、键、值都来自同一个序列
- 交叉注意力：查询来自一个序列，键、值来自另一个序列
- 多头注意力：并行计算多个注意力

## 5. 预训练语言模型

### 定义
预训练语言模型是在大规模无标注文本上预训练的模型。

### 代表性模型
- BERT：双向编码器
- GPT：单向生成模型
- T5：文本到文本转换
- RoBERTa：BERT的改进版本
- ALBERT：参数共享的BERT

### 预训练任务
- 掩码语言模型(MLM)
- 下一句预测(NSP)
- 语言建模

## 6. 文本分类

### 定义
将文本分配到预定义类别中的任务。

### 应用场景
- 情感分析
- 主题分类
- 垃圾邮件检测
- 语言识别
- 意图识别

### 方法
- 传统方法：基于规则、统计、机器学习
- 深度学习方法：CNN、RNN、Transformer
- 预训练模型：BERT、GPT等

## 7. 机器翻译

### 定义
使用计算机将一种自然语言自动翻译成另一种自然语言。

### 发展历程
- 基于规则的方法
- 统计机器翻译(SMT)
- 神经机器翻译(NMT)

### 神经机器翻译
- 编码器-解码器架构
- 注意力机制
- Transformer架构

## 8. 问答系统

### 定义
根据问题从文档中找出答案的系统。

### 类型
- 基于检索的问答
- 基于生成的问答
- 混合方法

### 技术
- 阅读理解
- 信息检索
- 答案生成
- 答案排序

## 9. 文本生成

### 定义
使用计算机生成自然语言文本。

### 应用场景
- 机器翻译
- 文本摘要
- 对话系统
- 内容创作

### 方法
- 基于规则的方法
- 基于统计的方法
- 基于神经网络的方法
- 基于预训练模型的方法

## 10. 评估指标

### 分类任务
- 准确率：正确分类的样本比例
- 精确率：预测为正类中真正为正类的比例
- 召回率：真正为正类中被正确预测的比例
- F1分数：精确率和召回率的调和平均

### 生成任务
- BLEU：基于n-gram匹配
- ROUGE：基于召回率
- METEOR：考虑同义词
- 人工评估：人类主观评价

### 翻译任务
- BLEU：n-gram匹配度
- METEOR：语义匹配
- ROUGE：召回率
- 人工评估：流畅度和忠实度
