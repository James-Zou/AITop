# 深度学习基础概念

## 1. 神经网络基础

### 感知机
感知机是最简单的神经网络，由Frank Rosenblatt在1957年提出。

**结构**:
- 输入层：接收输入特征
- 权重：每个输入对应一个权重
- 偏置：额外的偏置项
- 激活函数：通常是阶跃函数

**数学表示**:
```
f(x) = sign(w·x + b)
```

**局限性**:
- 只能解决线性可分问题
- 无法处理XOR等非线性问题
- 单层限制，表达能力有限

### 多层感知机(MLP)
多层感知机是由多个全连接层组成的神经网络。

**结构组成**:
1. 输入层：接收原始特征
2. 隐藏层：一个或多个中间层
3. 输出层：产生最终预测结果

**前向传播**:
```
h₁ = σ(W₁x + b₁)     # 第一隐藏层
h₂ = σ(W₂h₁ + b₂)    # 第二隐藏层
...
y = σ(Wₙhₙ₋₁ + bₙ)   # 输出层
```

**优势**:
- 可以逼近任意连续函数
- 能够学习复杂的非线性模式
- 结构简单，易于理解

## 2. 反向传播算法

### 工作原理
反向传播是训练神经网络的核心算法，通过计算损失函数对每个参数的梯度来更新网络参数。

**步骤**:
1. 前向传播：计算预测值
2. 计算损失：比较预测值和真实值
3. 反向传播：计算梯度
4. 参数更新：使用梯度更新参数

**数学表示**:
```
# 前向传播
z^(l) = W^(l)a^(l-1) + b^(l)
a^(l) = σ(z^(l))

# 反向传播
δ^(L) = ∇_a L ⊙ σ'(z^(L))
δ^(l) = ((W^(l+1))^T δ^(l+1)) ⊙ σ'(z^(l))

# 参数更新
W^(l) = W^(l) - α ∂L/∂W^(l)
b^(l) = b^(l) - α ∂L/∂b^(l)
```

## 3. 激活函数

### 常用激活函数

**Sigmoid函数**:
```
σ(x) = 1/(1 + e^(-x))
```
- 输出范围：(0,1)
- 缺点：梯度消失问题严重

**Tanh函数**:
```
tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))
```
- 输出范围：(-1,1)
- 比sigmoid梯度更大

**ReLU函数**:
```
ReLU(x) = max(0, x)
```
- 计算简单高效
- 解决梯度消失问题
- 缺点：负值区域梯度为0

**Leaky ReLU**:
```
LeakyReLU(x) = max(0.01x, x)
```
- 解决死神经元问题
- 保持ReLU的优势

## 4. 梯度问题

### 梯度消失
在深层网络中，梯度在反向传播过程中逐渐变小。

**原因**:
- 网络深度过深
- 激活函数导数小于1
- 权重初始化不当

**解决方案**:
- 使用ReLU激活函数
- 残差连接
- 批归一化
- 梯度裁剪

### 梯度爆炸
梯度在反向传播过程中逐渐变大。

**原因**:
- 权重初始化过大
- 学习率过大
- 网络结构问题

**解决方案**:
- 梯度裁剪
- 权重正则化
- 学习率调度
- 权重初始化

## 5. 正则化技术

### Dropout
随机将一部分神经元输出设为0。

**工作原理**:
- 训练时：以概率p随机丢弃神经元
- 推理时：使用所有神经元
- 作用：防止过拟合，提高泛化能力

### 批归一化
对每个批次的输入进行标准化。

**数学过程**:
```
μ_B = (1/m) ∑_{i=1}^m x_i
σ²_B = (1/m) ∑_{i=1}^m (x_i - μ_B)²
x̂_i = (x_i - μ_B)/√(σ²_B + ε)
y_i = γx̂_i + β
```

**作用**:
- 减少内部协变量偏移
- 改善梯度传播
- 允许使用更大的学习率

## 6. 优化算法

### 梯度下降
通过迭代更新参数来最小化损失函数。

**数学表示**:
```
θ_{t+1} = θ_t - α∇J(θ_t)
```

**变种**:
- 批量梯度下降：使用全部数据
- 随机梯度下降：使用单个样本
- 小批量梯度下降：使用小批量数据

### 自适应优化器

**Adam**:
结合动量和RMSprop的优点。

**RMSprop**:
使用指数移动平均调整学习率。

**AdaGrad**:
根据历史梯度调整学习率。

## 7. 损失函数

### 分类问题
**交叉熵损失**:
```
L = -∑ y_i log(ŷ_i)
```

**二元交叉熵**:
```
L = -[y log(ŷ) + (1-y) log(1-ŷ)]
```

### 回归问题
**均方误差**:
```
L = (1/n) ∑(y_i - ŷ_i)²
```

**平均绝对误差**:
```
L = (1/n) ∑|y_i - ŷ_i|
```

## 8. 网络架构

### 全连接网络
每层每个神经元都与下一层所有神经元连接。

**特点**:
- 参数量大
- 适合小规模数据
- 容易过拟合

### 卷积神经网络
使用卷积层处理图像数据。

**特点**:
- 参数共享
- 局部连接
- 平移不变性

### 循环神经网络
处理序列数据。

**特点**:
- 时间步连接
- 记忆能力
- 适合序列建模

## 9. 训练技巧

### 数据预处理
- 标准化：均值0，方差1
- 归一化：缩放到[0,1]范围
- 数据增强：增加训练样本

### 超参数调优
- 学习率：影响收敛速度
- 批次大小：影响训练稳定性
- 网络深度：影响表达能力
- 正则化参数：影响过拟合

### 早停
在验证集性能不再提升时停止训练。

### 学习率调度
动态调整学习率。

**策略**:
- 固定学习率
- 步长衰减
- 指数衰减
- 余弦退火

## 10. 评估和调试

### 评估指标
- 准确率：正确预测的比例
- 精确率：预测为正类中真正为正类的比例
- 召回率：真正为正类中被正确预测的比例
- F1分数：精确率和召回率的调和平均

### 调试技巧
- 检查梯度：监控梯度大小
- 可视化：绘制训练曲线
- 对比实验：控制变量实验
- 数据检查：验证数据质量

### 常见问题
- 过拟合：增加正则化，减少模型复杂度
- 欠拟合：增加模型复杂度，延长训练
- 训练不稳定：调整学习率，检查数据
- 收敛慢：调整学习率，检查初始化
